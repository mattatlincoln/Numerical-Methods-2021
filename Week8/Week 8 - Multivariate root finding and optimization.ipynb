{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.linalg as linalg\n",
    "import matplotlib.pyplot as plt\n",
    "# print out vectors with 3 decimal places\n",
    "np.set_printoptions(formatter={'float': '{: 6.3f}'.format})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finding roots of a vector valued function\n",
    "\n",
    "Consider a vector valued function $\\mathbf{F} (\\mathbf{x})$ : $\\mathbb{R}^N \\mapsto \\mathbb{R}^N$\n",
    "\n",
    "let's work with the case that the vector $\\mathbf{x} = (x_1, x_2, x_3)^T$ is mapped to\n",
    "\n",
    "$$\\mathbf{F} = (F_1, F_2, F_3)^T= (x_1^2, x_2^2, x_3^2)^T\n",
    "$$\n",
    "\n",
    "and we want to find the values of $\\mathbf{x}$ where $\\mathbf{F} (\\mathbf{x})  = \\mathbf{0}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example vector valued function that squares each component of the input vector\n",
    "def f(x):\n",
    "    return x**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 4.000,  4.000,  4.000])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# if we pass in an array as our model of a vector we get an array back \n",
    "# exactly as we want\n",
    "F = f(np.array([2.0,2.0,2.0]))\n",
    "F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Root finding\n",
    "\n",
    "To find the values of $\\mathbf{x}$ where $\\mathbf{F}(\\mathbf{x}) = \\mathbf{0}$ we need to find the locations where each component is zero. \n",
    "\n",
    "We can Taylor expand each component - for instance \n",
    "$$\n",
    "F_1(x_1+dx_1 + x_2+ dx_2 + x_1+dx_2) \\approx F_1(x_1, x_2, x_3) + \\frac{\\partial F_1}{\\partial x_1}dx_1 + \\frac{\\partial F_1}{\\partial x_2}dx_2 + \\frac{\\partial F_1}{\\partial x_3}dx_3 + \\cdots\n",
    "$$\n",
    "for component 1 and similarly for all others.\n",
    "\n",
    "We can write the set of equations as\n",
    "$$\n",
    "\\mathbf{F}(x_1+dx_1 + x_2+ dx_2 + x_1+dx_2) \\approx F_1(x_1, x_2, x_3) + \n",
    "\\begin{pmatrix}\n",
    "\\frac{\\partial F_1}{\\partial x_1} & \\frac{\\partial F_1}{\\partial x_2} & \\frac{\\partial F_1}{\\partial x_3}\\\\\n",
    "\\frac{\\partial F_2}{\\partial x_1} & \\frac{\\partial F_2}{\\partial x_2} & \\frac{\\partial F_2}{\\partial x_3}\\\\\n",
    "\\frac{\\partial F_3}{\\partial x_1} & \\frac{\\partial F_3}{\\partial x_2} & \\frac{\\partial F_3}{\\partial x_3}\\\\\n",
    "\\end{pmatrix}\n",
    "\\begin{pmatrix}\n",
    "dx_1 \\\\ dx_2 \\\\ dx_3\n",
    "\\end{pmatrix} + \\cdots\n",
    "$$\n",
    "\n",
    "where the partial derivatives are evaluated at the point $(x_1, x_2, x_3)$.\n",
    "\n",
    "To solve this we can use our normal methods of solving sets of equations - see example below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### example case\n",
    "\n",
    "In the example \n",
    "$$\n",
    "\\mathbf{F} = (F_1, F_2, F_3)^T= (x_1^2, x_2^2, x_3^2)^T\n",
    "$$ \n",
    "we have\n",
    "\n",
    "$$\n",
    "\\mathbf{F}(x_1+dx_1 + x_2+ dx_2 + x_1+dx_2) \\approx \\mathbf{F}(x_1, x_2, x_3) + \n",
    "\\begin{pmatrix}\n",
    "2 x_1 & 0 & 0 \\\\\n",
    "0 & 2 x_2 & 0 \\\\\n",
    "0 & 0 & 2 x_3\n",
    "\\end{pmatrix}\n",
    "\\begin{pmatrix}\n",
    "dx_1 \\\\ dx_2 \\\\ dx_3\n",
    "\\end{pmatrix} + \\cdots\n",
    "$$\n",
    "\n",
    "using a Newton type algorithm we want the LHS to equal zero giving us an update vector by solving the system of equations\n",
    "\n",
    "$$\n",
    "\\begin{pmatrix}\n",
    "2 x_1 & 0 & 0 \\\\\n",
    "0 & 2 x_2 & 0 \\\\\n",
    "0 & 0 & 2 x_3\n",
    "\\end{pmatrix}\n",
    "\\begin{pmatrix}\n",
    "dx_1 \\\\ dx_2 \\\\ dx_3\n",
    "\\end{pmatrix} = \n",
    "-\\begin{pmatrix}\n",
    "F_1(x_1, x_2, x_3) \\\\ F_2(x_1, x_2, x_3) \\\\ F_3(x_1, x_2, x_3)\n",
    "\\end{pmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.000,  0.000,  0.000])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# code up the root finder for our F \n",
    "\n",
    "x = np.array([3,3,3]) # initial point\n",
    "eps = 1e-6 # accuracy required\n",
    "count = 0\n",
    "\n",
    "while (np.max(x) > eps):\n",
    "    F = f(x)\n",
    "    gradF = np.array([\n",
    "        [2*x[0],0,0],\n",
    "        [0,2*x[1],0],\n",
    "        [0,0,2*x[2]]])\n",
    "    dx = linalg.solve(gradF,-F)\n",
    "    x = x + dx\n",
    "    # safety\n",
    "    count += 1\n",
    "    if count > 100: break\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extrema finding\n",
    "\n",
    "Extrema of a multivatiate function are found when the directed gradient (grad) of the function is zero - i.e. when all of the partial derivatives are zero. Note that this is what we did for linear least squared curve fitting.\n",
    "\n",
    "For a function that maps $f(\\mathbf{x}) :\\mathbb{R}^N \\mapsto \\mathbb{R}$ we have the condition for an extrema given by\n",
    "$\\mathrm{grad} f(\\mathbf{x}) = \\nabla f (\\mathbf{x}) = \\mathbf{0}$. \n",
    "\n",
    "If we take the function $f = 1/3 (x_1^3 + x_2^3 + x_3^3)$ then we see that $$\\nabla f = \\left( \\frac{\\partial f}{\\partial x_1}, \\frac{\\partial f}{\\partial x_2} , \\frac{\\partial f}{\\partial x_3} \\right)^T = (x_1^2, x_2^2 , x_3^2)^T$$ and the condition that this is equal to $\\mathbf{0}$ is exactly the same problem that we just solved.\n",
    "\n",
    "In terms of our function $f$ the problem is\n",
    "\n",
    "$$\n",
    "\\nabla f(x_1+dx_1 + x_2+ dx_2 + x_1+dx_2) = \\mathbf{0} \\approx \\nabla f(x_1, x_2, x_3) + \n",
    "\\begin{pmatrix}\n",
    "\\frac{\\partial ^2 f}{\\partial x_1^2} & \\frac{\\partial ^2 f}{\\partial x_1 \\partial x_2} & \\frac{\\partial ^2 f}{\\partial x_1 \\partial x_3}\\\\\n",
    "\\frac{\\partial ^2 f}{\\partial x_2 \\partial x_1} & \\frac{\\partial ^2 f}{\\partial x_2^2} & \\frac{\\partial ^2 f}{\\partial x_2 \\partial x_3}\\\\\n",
    "\\frac{\\partial ^2 f}{\\partial x_3 \\partial x_1} & \\frac{\\partial ^2 f}{\\partial x_3 \\partial x_2} & \\frac{\\partial ^2 f}{\\partial x_3^2}\\\\\n",
    "\\end{pmatrix}\n",
    "\\begin{pmatrix}\n",
    "dx_1 \\\\ dx_2 \\\\ dx_3\n",
    "\\end{pmatrix} + \\cdots\n",
    "$$\n",
    "\n",
    "where we have made a Taylor expansion of the function to 1st order in $\\mathbf{dx}$ the change in $\\mathbf{x}$.\n",
    "\n",
    "The matrix of 2nd derivatives is called the *Hessian* matrix of $f$. The gradient vector, $\\textrm{grad}(f)$ or $\\nabla f$, is also called the *Jacobian* of $f$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## another example\n",
    "\n",
    "find an extremum of the function $f(x_1,x_2,x_3) = x_1 x_2 x_3$.\n",
    "\n",
    "In this case the gradient matrix will be given by \n",
    "$$\n",
    "\\nabla f = \\left( \\frac{\\partial f}{\\partial x_1}, \\frac{\\partial f}{\\partial x_2} , \\frac{\\partial f}{\\partial x_3} \\right)^T = (x_2 x_3, x_1 x_3, x_1 x_2)^T\n",
    "$$\n",
    "\n",
    "and the update step to find the value where $\\nabla f = \\mathbf{0}$ will be given by\n",
    "\n",
    "$$\n",
    "\\begin{pmatrix}\n",
    "\\frac{\\partial ^2 f}{\\partial x_1^2} & \\frac{\\partial ^2 f}{\\partial x_1 \\partial x_2} & \\frac{\\partial ^2 f}{\\partial x_1 \\partial x_3}\\\\\n",
    "\\frac{\\partial ^2 f}{\\partial x_2 \\partial x_1} & \\frac{\\partial ^2 f}{\\partial x_2^2} & \\frac{\\partial ^2 f}{\\partial x_2 \\partial x_3}\\\\\n",
    "\\frac{\\partial ^2 f}{\\partial x_3 \\partial x_1} & \\frac{\\partial ^2 f}{\\partial x_3 \\partial x_2} & \\frac{\\partial ^2 f}{\\partial x_3^2}\\\\\n",
    "\\end{pmatrix}\n",
    "\\begin{pmatrix}\n",
    "dx_1 \\\\ dx_2 \\\\ dx_3\n",
    "\\end{pmatrix} \n",
    "=\n",
    "\\begin{pmatrix}\n",
    "0 & x_3 & x_2 \\\\\n",
    "x_3 & 0 & x_1 \\\\\n",
    "x_2 & x_1 & 0 \n",
    "\\end{pmatrix}\n",
    "\\begin{pmatrix}\n",
    "dx_1 \\\\ dx_2 \\\\ dx_3\n",
    "\\end{pmatrix} \n",
    "= -\\nabla f(x_1, x_2, x_3) =  -(x_2 x_3, x_1 x_3, x_1 x_2)^T\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code up the extremum finder for our F \n",
    "def gradg(x):\n",
    "    \"\"\"hard coded grad of f = x_1 x_2 x_3\"\"\"\n",
    "    return np.array([x[1]*x[2], x[0]*x[2], x[0]*x[1]])\n",
    "\n",
    "def Hessg(x):\n",
    "    \"\"\"hard coded Hessian of f = x_1 x_2 x_3\"\"\"\n",
    "    return np.array([\n",
    "        [0   ,  x[2],  x[1]],\n",
    "        [x[2],  0   ,  x[0]],\n",
    "        [x[1],  x[0],  0  ]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x = [ 3.000  5.000  9.000];  gradient = [ 45.000  27.000  15.000];   f = 135.000;   after 0 iterations\n",
      "x = [ 1.500  2.500  4.500];  gradient = [ 11.250  6.750  3.750];   f = 16.875;   after 1 iterations\n",
      "x = [ 0.750  1.250  2.250];  gradient = [ 2.812  1.688  0.938];   f =  2.109;   after 2 iterations\n",
      "x = [ 0.375  0.625  1.125];  gradient = [ 0.703  0.422  0.234];   f =  0.264;   after 3 iterations\n",
      "x = [ 0.188  0.312  0.562];  gradient = [ 0.176  0.105  0.059];   f =  0.033;   after 4 iterations\n",
      "x = [ 0.094  0.156  0.281];  gradient = [ 0.044  0.026  0.015];   f =  0.004;   after 5 iterations\n",
      "x = [ 0.047  0.078  0.141];  gradient = [ 0.011  0.007  0.004];   f =  0.001;   after 6 iterations\n",
      "x = [ 0.023  0.039  0.070];  gradient = [ 0.003  0.002  0.001];   f =  0.000;   after 7 iterations\n",
      "x = [ 0.012  0.020  0.035];  gradient = [ 0.001  0.000  0.000];   f =  0.000;   after 8 iterations\n",
      "x = [ 0.006  0.010  0.018];  gradient = [ 0.000  0.000  0.000];   f =  0.000;   after 9 iterations\n",
      "x = [ 0.003  0.005  0.009];  gradient = [ 0.000  0.000  0.000];   f =  0.000;   after 10 iterations\n",
      "x = [ 0.001  0.002  0.004];  gradient = [ 0.000  0.000  0.000];   f =  0.000;   after 11 iterations\n",
      "x = [ 0.001  0.001  0.002];  gradient = [ 0.000  0.000  0.000];   f =  0.000;   after 12 iterations\n",
      "x = [ 0.000  0.001  0.001];  gradient = [ 0.000  0.000  0.000];   f =  0.000;   after 13 iterations\n",
      "x = [ 0.000  0.000  0.001];  gradient = [ 0.000  0.000  0.000];   f =  0.000;   after 14 iterations\n",
      "x = [ 0.000  0.000  0.000];  gradient = [ 0.000  0.000  0.000];   f =  0.000;   after 15 iterations\n",
      "x = [ 0.000  0.000  0.000];  gradient = [ 0.000  0.000  0.000];   f =  0.000;   after 16 iterations\n",
      "x = [ 0.000  0.000  0.000];  gradient = [ 0.000  0.000  0.000];   f =  0.000;   after 17 iterations\n"
     ]
    }
   ],
   "source": [
    "x = np.array([3.0,5.0,9.0]) # initial point\n",
    "eps = 1e-4 # accuracy required\n",
    "count = 0\n",
    "while (np.max(x) > eps):\n",
    "    grad = gradg(x)\n",
    "    hessg = Hessg(x)\n",
    "    dx = linalg.solve(hessg, -grad)\n",
    "    print(\"x = {};  gradient = {};   f = {:6.3f};   after {} iterations\".format(x, grad, np.product(x), count))\n",
    "    x = x + dx\n",
    "    # safety\n",
    "    count += 1\n",
    "    if count > 100: break\n",
    "\n",
    "print(\"x = {};  gradient = {};   f = {:6.3f};   after {} iterations\".format(x, grad, np.product(x), count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating the Hessian and grad\n",
    "\n",
    "It can get very hard to work out analytically / code / computationally expensive if there are many variables. Like the secant approximation in 1-D, finite differences can provide an easier option for the first 2 problems."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
